"""
Musubi Tuner Qwen Image LoRA Training Config Template

Generates TOML dataset configuration files for qwen_image_train_network.py
Supports Qwen-Image (text-to-image) and Qwen-Image-Edit variants (image editing)
"""

import os


def generate_dataset_config(
    image_folder: str,
    resolution: int = 1024,
    batch_size: int = 1,
    enable_bucket: bool = True,
    num_repeats: int = 10,
    control_directory: str = None,
) -> str:
    """
    Generate a TOML dataset config file for Musubi Tuner Qwen Image training.

    Args:
        image_folder: Path to target images (the output/result images)
        num_repeats: How many times to repeat each image per epoch.
                     Higher = fewer epochs for same step count, less overhead.
        control_directory: Path to control/source images for Qwen-Image-Edit training.
                          Control images are matched by basename with target images.
    Returns the config as a TOML string.
    """

    # Escape backslashes for TOML on Windows
    image_folder_escaped = image_folder.replace('\\', '/')

    config = f'''# Musubi Tuner Qwen Image Dataset Config
# Generated by ComfyUI Musubi Qwen Image LoRA Trainer

[general]
resolution = [{resolution}, {resolution}]
batch_size = {batch_size}
enable_bucket = {str(enable_bucket).lower()}
caption_extension = ".txt"

[[datasets]]
image_directory = "{image_folder_escaped}"
num_repeats = {num_repeats}
'''

    # Add control_directory for Qwen-Image-Edit training
    if control_directory:
        control_dir_escaped = control_directory.replace('\\', '/')
        config += f'control_directory = "{control_dir_escaped}"\n'

    return config


def save_config(config_content: str, config_path: str):
    """Save config content to a TOML file."""
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write(config_content)


# VRAM mode presets for Qwen Image (Musubi Tuner)
# Uses fp8_vl for text encoder (not fp8_llm like Z-Image)
# blocks_to_swap is now a separate parameter (0-45)
# Note: Musubi Tuner ALWAYS requires pre-caching latents and text encoder outputs
MUSUBI_QWEN_IMAGE_VRAM_PRESETS = {
    "Max (1024px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": False,
        "fp8_scaled": False,
        "fp8_vl": False,
        "resolution": 1024,
    },
    "Max (1024px) fp8": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": False,
        "fp8_scaled": True,
        "fp8_vl": True,
        "resolution": 1024,
    },
    "Medium (768px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": False,
        "fp8_vl": False,
        "resolution": 768,
    },
    "Medium (768px) fp8": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_vl": True,
        "resolution": 768,
    },
    "Low (512px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": False,
        "fp8_vl": False,
        "resolution": 512,
    },
    "Low (512px) fp8": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_vl": True,
        "resolution": 512,
    },
}
